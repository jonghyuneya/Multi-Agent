"""
LangGraph implementation for the Korean closing market briefing pipeline.

Defines nodes and graph structure for:
- load_sources: Load data from source files
- script_writer_with_tools: Generate script using tool calls for data access
- critic: Review and critique the script
- revision_writer: Revise script based on feedback
"""

import json
import logging
from typing import Literal, Dict, Any, List
from datetime import datetime
from pathlib import Path

from openai import OpenAI
from langgraph.graph import StateGraph, END

from closing_briefing.config import Config, InvalidLLMJSONError
from closing_briefing.models import (
    ClosingBriefingState,
    ExtractedFact,
    EarningsResult,
    NewsItem,
    UpcomingEvent,
    MacroIndicator,
    CriticFeedback,
    ChecklistItem,
    HallucinationItem,
    Reference,
)
from closing_briefing.prompts import (
    SCRIPT_WRITER_WITH_TOOLS_SYSTEM_PROMPT,
    CRITIC_WITH_TOOLS_SYSTEM_PROMPT,
    REVISION_WRITER_SYSTEM_PROMPT,
)
from closing_briefing.data_loader import ClosingBriefingDataLoader
from closing_briefing.tools import BRIEFING_TOOLS, DataToolExecutor, format_tool_result_for_llm

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# LLM Wrapper with Tool Support
# ============================================================================

def call_llm(
    system_prompt: str,
    user_content: str,
    response_format: Literal["json", "text"] = "text",
    model: str = None,
    temperature: float = None,
) -> str:
    """
    Call the OpenAI API with the given system prompt and user content.
    """
    Config.validate()
    
    client = OpenAI(api_key=Config.OPENAI_API_KEY)
    
    model = model or Config.OPENAI_MODEL
    temperature = temperature if temperature is not None else Config.OPENAI_TEMPERATURE
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]
    
    kwargs = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": Config.MAX_TOKENS,
    }
    
    if response_format == "json":
        kwargs["response_format"] = {"type": "json_object"}
    
    try:
        logger.info(f"Calling LLM with model={model}, response_format={response_format}")
        response = client.chat.completions.create(**kwargs)
        content = response.choices[0].message.content
        
        if response_format == "json":
            try:
                json.loads(content)
            except json.JSONDecodeError as e:
                raise InvalidLLMJSONError(
                    f"LLM returned invalid JSON: {str(e)}",
                    raw_response=content
                )
        
        return content
        
    except Exception as e:
        logger.error(f"Error calling LLM: {str(e)}")
        raise


def call_llm_with_tools(
    system_prompt: str,
    user_content: str,
    tools: List[Dict],
    tool_executor: DataToolExecutor,
    model: str = None,
    temperature: float = None,
    max_tool_calls: int = 20,
) -> tuple[str, List[Reference]]:
    """
    Call the OpenAI API with tool support.
    
    The LLM can call tools to retrieve data, and we execute those tools
    and feed results back until the LLM produces a final response.
    
    Args:
        system_prompt: System prompt for the LLM
        user_content: User message content
        tools: List of tool definitions
        tool_executor: DataToolExecutor instance to execute tool calls
        model: Model to use
        temperature: Temperature setting
        max_tool_calls: Maximum number of tool call rounds
        
    Returns:
        Tuple of (final_response, list_of_references)
    """
    Config.validate()
    
    client = OpenAI(api_key=Config.OPENAI_API_KEY)
    
    model = model or Config.OPENAI_MODEL
    temperature = temperature if temperature is not None else Config.OPENAI_TEMPERATURE
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]
    
    all_references: List[Reference] = []
    tool_call_count = 0
    
    while tool_call_count < max_tool_calls:
        logger.info(f"Calling LLM with tools (round {tool_call_count + 1})")
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice="auto",
            temperature=temperature,
            max_tokens=Config.MAX_TOKENS,
        )
        
        message = response.choices[0].message
        
        # Check if the model wants to call tools
        if message.tool_calls:
            # Add assistant message with tool calls
            messages.append(message)
            
            # Process each tool call
            for tool_call in message.tool_calls:
                tool_name = tool_call.function.name
                try:
                    arguments = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError:
                    arguments = {}
                
                logger.info(f"Executing tool: {tool_name} with args: {arguments}")
                
                # Execute the tool
                result = tool_executor.execute_tool(tool_name, arguments)
                
                # Collect references
                for ref_dict in result.get('references', []):
                    all_references.append(Reference(
                        source_type=ref_dict.get('source_type', ''),
                        source_file=ref_dict.get('source_file', ''),
                        quote=ref_dict.get('quote', ''),
                        provider=ref_dict.get('provider'),
                        date=ref_dict.get('date'),
                    ))
                
                # Format result for LLM
                result_str = format_tool_result_for_llm(result)
                
                # Add tool result message
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": result_str,
                })
            
            tool_call_count += 1
        else:
            # No more tool calls, return the final response
            return message.content, all_references
    
    # If we hit max tool calls, get final response without tools
    logger.warning(f"Hit max tool calls ({max_tool_calls}), forcing final response")
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=Config.MAX_TOKENS,
    )
    
    return response.choices[0].message.content, all_references


# ============================================================================
# Node Functions
# ============================================================================

def load_sources_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Load data from source files.
    """
    logger.info("=== Node: load_sources ===")
    
    source_path = state.sources.get('_source_path', 'data/sample_sources')
    load_news_from_dynamodb = state.sources.get('_load_news_from_dynamodb', True)
    dynamodb_profile = state.sources.get('_dynamodb_profile', 'jonghyun')
    
    loader = ClosingBriefingDataLoader(
        source_path,
        load_news_from_dynamodb=load_news_from_dynamodb,
        dynamodb_profile=dynamodb_profile,
    )
    sources = loader.load_all_sources()
    
    briefing_date = state.briefing_date
    if not briefing_date:
        market_summary = sources.get('market_summary', {})
        briefing_date = market_summary.get('date', datetime.now().strftime('%Y-%m-%d'))
    
    logger.info(f"Loaded sources for briefing date: {briefing_date}")
    logger.info(f"Sources loaded: macro={len(sources.get('macro_data', []))}, "
                f"earnings={len(sources.get('earnings_data', []))}, "
                f"news={len(sources.get('news_data', []))}, "
                f"calendar={len(sources.get('calendar_events', []))}")
    
    return {
        "sources": sources,
        "briefing_date": briefing_date,
    }


def script_writer_with_tools_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Generate the closing script using tool calls to access data.
    
    The LLM will call tools to retrieve specific data and include
    exact references in the output.
    """
    logger.info("=== Node: script_writer_with_tools ===")
    
    sources = state.sources
    briefing_date = state.briefing_date
    
    # Create tool executor with loaded sources
    tool_executor = DataToolExecutor(sources, briefing_date)
    
    # Build initial context for the LLM
    user_message = f"""ì˜¤ëŠ˜ ë‚ ì§œ: {briefing_date}

ì¥ë§ˆê° ë¸Œë¦¬í•‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°:
- ê±°ì‹œê²½ì œ ì§€í‘œ: {len(sources.get('macro_data', []))}ê°œ
- ê²½ì œ ì¼ì •: {len(sources.get('calendar_events', []))}ê°œ
- ë‰´ìŠ¤: {len(sources.get('news_data', []))}ê°œ
- ì‹¤ì  ë°œí‘œ: {len(sources.get('earnings_data', []))}ê°œ
- FOMC ì´ë²¤íŠ¸: {len(sources.get('fomc_events', []))}ê°œ

ë„êµ¬(tools)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ , ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.
ëª¨ë“  ìˆ«ìì™€ ì‚¬ì‹¤ì€ ë°˜ë“œì‹œ ë„êµ¬ë¥¼ í†µí•´ í™•ì¸í•œ ë°ì´í„°ì—ì„œ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.

ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì‹œ:
1. ë¨¼ì € get_market_summaryë¡œ ì‹œì¥ ê°œìš”ë¥¼ í™•ì¸í•˜ì„¸ìš”
2. get_news_articlesë¡œ ì£¼ìš” ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”
3. get_earnings_resultsë¡œ ì‹¤ì  ë°œí‘œë¥¼ í™•ì¸í•˜ì„¸ìš”
4. get_calendar_eventsë¡œ í–¥í›„ ì¼ì •ì„ í™•ì¸í•˜ì„¸ìš”
5. í•„ìš”ì‹œ get_macro_indicatorsë¡œ ê±°ì‹œì§€í‘œë¥¼ í™•ì¸í•˜ì„¸ìš”

ê° ì •ë³´ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [REF: source_type | "ì •í™•í•œ ì¸ìš©ë¬¸"] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”."""

    try:
        response, references = call_llm_with_tools(
            system_prompt=SCRIPT_WRITER_WITH_TOOLS_SYSTEM_PROMPT,
            user_content=user_message,
            tools=BRIEFING_TOOLS,
            tool_executor=tool_executor,
        )
        
        logger.info(f"Generated script draft ({len(response)} chars) with {len(references)} references")
        
        # Extract keywords from the script (simple extraction)
        keywords = _extract_keywords_from_script(response)
        
        return {
            "script_draft": response,
            "keywords": keywords,
            "references": references,
        }
        
    except Exception as e:
        logger.error(f"Error in script_writer_with_tools: {str(e)}")
        return {
            "script_draft": None,
            "error_message": str(e),
        }


def _extract_keywords_from_script(script: str) -> List[str]:
    """Extract keywords from the script content."""
    keywords = []
    
    # Common market themes to look for
    theme_patterns = [
        ("AI", "AI"),
        ("ì¸ê³µì§€ëŠ¥", "AI"),
        ("ë°˜ë„ì²´", "ë°˜ë„ì²´"),
        ("ê¸ˆë¦¬", "ê¸ˆë¦¬"),
        ("ì—°ì¤€", "ì—°ì¤€"),
        ("Fed", "ì—°ì¤€"),
        ("ì¸í”Œë ˆì´ì…˜", "ì¸í”Œë ˆì´ì…˜"),
        ("ì‹¤ì ", "ì‹¤ì "),
        ("ê¸°ìˆ ì£¼", "ê¸°ìˆ ì£¼"),
        ("ê²½ê¸°ì¹¨ì²´", "ê²½ê¸°ì¹¨ì²´"),
        ("ê³ ìš©", "ê³ ìš©"),
    ]
    
    for pattern, keyword in theme_patterns:
        if pattern in script and keyword not in keywords:
            keywords.append(keyword)
        if len(keywords) >= 3:
            break
    
    if not keywords:
        keywords = ["ì‹œì¥ ë™í–¥"]
    
    return keywords


def critic_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Review and critique the script draft using tool calls to verify data.
    """
    logger.info("=== Node: critic (with tools) ===")
    
    if not state.script_draft:
        logger.error("No script draft to critique")
        return {
            "critic_feedback": None,
            "error_message": "No script draft available for critique",
        }
    
    sources = state.sources
    briefing_date = state.briefing_date
    
    # Create tool executor for verification
    tool_executor = DataToolExecutor(sources, briefing_date)
    
    # Build user message for critic
    user_message = f"""## ê²€ì¦í•  ëŒ€ë³¸:

{state.script_draft}

---

## ê²€ì¦ ì§€ì‹œì‚¬í•­:

1. ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”
2. ëŒ€ë³¸ì˜ ê° ì‚¬ì‹¤ì„ ì›ë³¸ ë°ì´í„°ì™€ ëŒ€ì¡°í•˜ì„¸ìš”
3. ë¶ˆì¼ì¹˜í•˜ê±°ë‚˜ ì¶œì²˜ê°€ ì—†ëŠ” ì •ë³´ë¥¼ í™˜ê°ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ê²€ì¦ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”

ê²€ì¦í•  í•­ëª©:
- ê±°ì‹œê²½ì œ ì§€í‘œ (get_macro_indicators)
- ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (get_news_articles)
- ê²½ì œ ì¼ì • (get_calendar_events)
- FOMC ì •ë³´ (get_fomc_events)
- ì‹¤ì  ìˆ˜ì¹˜ (get_earnings_results)
"""

    try:
        response, _ = call_llm_with_tools(
            system_prompt=CRITIC_WITH_TOOLS_SYSTEM_PROMPT,
            user_content=user_message,
            tools=BRIEFING_TOOLS,
            tool_executor=tool_executor,
        )
        
        critic_feedback = _parse_critic_response(response)
        
        logger.info(f"Critic evaluation complete: {critic_feedback.overall_quality}")
        
        new_iterations = state.iterations + 1
        
        return {
            "critic_feedback": critic_feedback,
            "iterations": new_iterations,
        }
        
    except Exception as e:
        logger.error(f"Error in critic: {str(e)}")
        return {
            "critic_feedback": None,
            "error_message": str(e),
        }


def _parse_critic_response(response: str) -> CriticFeedback:
    """Parse the critic's text response into structured feedback."""
    
    # Extract summary evaluation
    summary_start = response.find("### ìš”ì•½ í‰ê°€")
    summary_end = response.find("### í•µì‹¬ ê²€ì¦ ê²°ê³¼")
    if summary_end == -1:
        summary_end = response.find("### ì²´í¬ë¦¬ìŠ¤íŠ¸")
    if summary_end == -1:
        summary_end = response.find("### ë‚´ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸")
    
    summary_evaluation = ""
    if summary_start != -1:
        if summary_end != -1:
            summary_evaluation = response[summary_start + len("### ìš”ì•½ í‰ê°€"):summary_end].strip()
        else:
            summary_evaluation = response[summary_start + len("### ìš”ì•½ í‰ê°€"):].strip()[:500]
    
    def parse_checklist_item(text: str, item_name: str) -> ChecklistItem:
        status = "ì¶©ì¡±"
        if "ì‹¬ê°í•œ ë¯¸í¡" in text:
            status = "ì‹¬ê°í•œ ë¯¸í¡"
        elif "ë¯¸í¡" in text:
            status = "ë¯¸í¡"
        
        explanation = text
        for s in ["**ì¶©ì¡±**", "**ë¯¸í¡**", "**ì‹¬ê°í•œ ë¯¸í¡**", "ì¶©ì¡±", "ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]:
            if s in explanation:
                parts = explanation.split(s, 1)
                if len(parts) > 1:
                    explanation = parts[1].strip().lstrip('-').strip()
                    break
        
        return ChecklistItem(
            item_name=item_name,
            status=status,
            explanation=explanation[:500]
        )
    
    # Extract critical validation checks
    hallucination_check = ChecklistItem(
        item_name="1. í™˜ê°(Hallucination) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    timeliness_check = ChecklistItem(
        item_name="2. ì‹œì˜ì„±(Timeliness) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    value_check = ChecklistItem(
        item_name="3. ì •ë³´ ê°€ì¹˜(Value) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    source_citation_check = ChecklistItem(
        item_name="4. ì¶œì²˜ ëª…ì‹œ(Source Citation) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    
    critical_start = response.find("### í•µì‹¬ ê²€ì¦ ê²°ê³¼")
    critical_end = response.find("### ë‚´ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸")
    if critical_end == -1:
        critical_end = response.find("### ìŠ¤íƒ€ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    
    if critical_start != -1:
        if critical_end != -1:
            critical_text = response[critical_start:critical_end]
        else:
            critical_text = response[critical_start:critical_start + 1500]
        
        lines = critical_text.split('\n')
        for line in lines:
            line = line.strip()
            if 'í™˜ê°' in line or 'Hallucination' in line:
                hallucination_check = parse_checklist_item(line, "1. í™˜ê°(Hallucination) ê²€ì¦")
            elif 'ì‹œì˜ì„±' in line or 'Timeliness' in line:
                timeliness_check = parse_checklist_item(line, "2. ì‹œì˜ì„±(Timeliness) ê²€ì¦")
            elif 'ì •ë³´ ê°€ì¹˜' in line or 'Value' in line:
                value_check = parse_checklist_item(line, "3. ì •ë³´ ê°€ì¹˜(Value) ê²€ì¦")
            elif 'ì¶œì²˜' in line or 'Source Citation' in line:
                source_citation_check = parse_checklist_item(line, "4. ì¶œì²˜ ëª…ì‹œ(Source Citation) ê²€ì¦")
    
    # Extract hallucinations found
    hallucinations_found = []
    halluc_start = response.find("### í™˜ê° ë°œê²¬ ëª©ë¡")
    halluc_end = response.find("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ")
    if halluc_start != -1:
        if halluc_end != -1:
            halluc_text = response[halluc_start:halluc_end]
        else:
            halluc_text = response[halluc_start:halluc_start + 1000]
        
        if "í™˜ê° ì—†ìŒ" not in halluc_text:
            lines = halluc_text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('-') and 'â†’' in line:
                    parts = line.split('â†’')
                    if len(parts) >= 2:
                        hallucinations_found.append(HallucinationItem(
                            fabricated_content=parts[0].lstrip('-').strip(),
                            explanation=parts[1].strip()
                        ))
    
    # Extract content checklist
    checklist = []
    content_start = response.find("### ë‚´ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸")
    content_end = response.find("### ìŠ¤íƒ€ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    if content_start != -1:
        if content_end != -1:
            content_text = response[content_start:content_end]
        else:
            content_text = response[content_start:content_start + 1500]
        
        lines = content_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') and ('ì¶©ì¡±' in line or 'ë¯¸í¡' in line):
                if ':' in line:
                    parts = line.split(':', 1)
                    item_name = parts[0].replace('-', '').strip()
                    explanation = parts[1].strip() if len(parts) > 1 else ""
                    status = "ì¶©ì¡±" if "ì¶©ì¡±" in line and "ë¯¸í¡" not in line else "ë¯¸í¡"
                    checklist.append(ChecklistItem(
                        item_name=item_name,
                        status=status,
                        explanation=explanation[:500]
                    ))
    
    # Extract style checklist
    style_start = response.find("### ìŠ¤íƒ€ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    style_end = response.find("### í™˜ê° ë°œê²¬ ëª©ë¡")
    if style_end == -1:
        style_end = response.find("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ")
    
    if style_start != -1:
        if style_end != -1:
            style_text = response[style_start:style_end]
        else:
            style_text = response[style_start:style_start + 1500]
        
        lines = style_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') and ('ì¶©ì¡±' in line or 'ë¯¸í¡' in line):
                if ':' in line:
                    parts = line.split(':', 1)
                    item_name = parts[0].replace('-', '').strip()
                    explanation = parts[1].strip() if len(parts) > 1 else ""
                    status = "ì¶©ì¡±" if "ì¶©ì¡±" in line and "ë¯¸í¡" not in line else "ë¯¸í¡"
                    checklist.append(ChecklistItem(
                        item_name=item_name,
                        status=status,
                        explanation=explanation[:500]
                    ))
    
    # Extract specific suggestions
    suggestions = []
    suggestions_start = response.find("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ")
    if suggestions_start != -1:
        suggestions_text = response[suggestions_start + len("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ"):]
        lines = suggestions_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') or line.startswith('â€¢'):
                suggestion = line.lstrip('-â€¢').strip()
                if suggestion:
                    suggestions.append(suggestion)
    
    # Determine overall quality
    critical_issues = 0
    if hallucination_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        critical_issues += 2
    elif hallucination_check.status == "ë¯¸í¡":
        critical_issues += 1
    if timeliness_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        critical_issues += 2
    elif timeliness_check.status == "ë¯¸í¡":
        critical_issues += 1
    if value_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        critical_issues += 2
    elif value_check.status == "ë¯¸í¡":
        critical_issues += 1
    
    miheup_count = sum(1 for item in checklist if "ë¯¸í¡" in item.status)
    
    if critical_issues >= 2 or hallucinations_found:
        overall_quality = "ì‹¬ê°"
    elif critical_issues == 1 or miheup_count >= 4:
        overall_quality = "ë¯¸í¡"
    elif miheup_count >= 2:
        overall_quality = "ë³´í†µ"
    elif miheup_count == 1:
        overall_quality = "ì–‘í˜¸"
    else:
        overall_quality = "ìš°ìˆ˜"
    
    return CriticFeedback(
        summary_evaluation=summary_evaluation or "í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        hallucination_check=hallucination_check,
        timeliness_check=timeliness_check,
        value_check=value_check,
        source_citation_check=source_citation_check,
        hallucinations_found=hallucinations_found,
        checklist=checklist,
        specific_suggestions=suggestions,
        overall_quality=overall_quality
    )


def revision_writer_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Revise the script based on critic feedback using tools.
    """
    logger.info("=== Node: revision_writer ===")
    
    if state.critic_feedback:
        if state.critic_feedback.hallucinations_found:
            logger.warning(f"Revising to fix {len(state.critic_feedback.hallucinations_found)} hallucinations")
    
    if not state.script_draft or not state.critic_feedback:
        logger.error("Missing script draft or critic feedback for revision")
        return {
            "script_revised": state.script_draft,
            "error_message": "Missing data for revision",
        }
    
    sources = state.sources
    briefing_date = state.briefing_date
    
    # Create tool executor for revision
    tool_executor = DataToolExecutor(sources, briefing_date)
    
    # Build user message with feedback
    user_message = f"""## ìˆ˜ì •ì´ í•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸:

{state.script_draft}

## ë¹„í‰ í”¼ë“œë°±:

### ìš”ì•½ í‰ê°€:
{state.critic_feedback.summary_evaluation}

### ì „ì²´ í’ˆì§ˆ: {state.critic_feedback.overall_quality}

### ìˆ˜ì • ì œì•ˆ:
{chr(10).join('- ' + s for s in state.critic_feedback.specific_suggestions)}

### ë°œê²¬ëœ í™˜ê° (í—ˆìœ„ ì •ë³´):
{chr(10).join('- ' + h.fabricated_content + ' â†’ ' + h.explanation for h in state.critic_feedback.hallucinations_found) if state.critic_feedback.hallucinations_found else 'ì—†ìŒ'}

---

ìœ„ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì •ë³´ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê³ , ì •í™•í•œ ì¶œì²˜ì™€ í•¨ê»˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    try:
        response, references = call_llm_with_tools(
            system_prompt=REVISION_WRITER_SYSTEM_PROMPT,
            user_content=user_message,
            tools=BRIEFING_TOOLS,
            tool_executor=tool_executor,
        )
        
        logger.info(f"Generated revised script ({len(response)} chars)")
        
        # Merge new references with existing ones
        all_references = list(state.references) if state.references else []
        all_references.extend(references)
        
        return {
            "script_revised": response,
            "script_draft": response,
            "references": all_references,
        }
        
    except Exception as e:
        logger.error(f"Error in revision_writer: {str(e)}")
        return {
            "script_revised": state.script_draft,
            "error_message": str(e),
        }


# ============================================================================
# Conditional Edge Logic
# ============================================================================

def _check_needs_revision(critic_feedback: CriticFeedback) -> tuple[bool, str]:
    """Check if the script needs revision based on critic feedback."""
    if critic_feedback.hallucinations_found:
        return True, f"í™˜ê° ë°œê²¬: {len(critic_feedback.hallucinations_found)}ê°œì˜ í—ˆìœ„ ì •ë³´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    if critic_feedback.hallucination_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]:
        return True, f"í™˜ê° ê²€ì¦ ë¯¸í¡: {critic_feedback.hallucination_check.explanation}"
    
    if critic_feedback.timeliness_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]:
        return True, f"ì‹œì˜ì„± ê²€ì¦ ë¯¸í¡: {critic_feedback.timeliness_check.explanation}"
    
    if critic_feedback.value_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        return True, f"ì •ë³´ ê°€ì¹˜ ì‹¬ê°í•˜ê²Œ ë¯¸í¡: {critic_feedback.value_check.explanation}"
    
    if critic_feedback.overall_quality in ["ì‹¬ê°", "ë¯¸í¡"]:
        return True, f"ì „ì²´ í’ˆì§ˆ ë¯¸í¡: {critic_feedback.overall_quality}"
    
    critical_failures = sum(
        1 for item in critic_feedback.checklist 
        if "ë¯¸í¡" in item.status and any(
            keyword in item.item_name.lower() 
            for keyword in ["í‚¤ì›Œë“œ", "ì‹¤ì ", "ë‰´ìŠ¤", "ì´ë²¤íŠ¸"]
        )
    )
    
    if critical_failures >= 2:
        return True, f"í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© {critical_failures}ê°œ ë¯¸í¡"
    
    return False, "í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±"


def should_continue_revision(state: ClosingBriefingState) -> Literal["revision_writer", "end"]:
    """Determine whether to continue with revision or end."""
    if not state.script_draft:
        logger.warning("No script draft available, ending workflow")
        return "end"
    
    if not state.critic_feedback:
        logger.warning("No critic feedback available, ending workflow")
        return "end"
    
    if state.iterations > state.max_iterations:
        logger.info("Max iterations reached, ending workflow")
        return "end"
    
    needs_revision, reason = _check_needs_revision(state.critic_feedback)
    
    if needs_revision:
        logger.info(f"Revision needed: {reason} (iteration {state.iterations}/{state.max_iterations})")
        return "revision_writer"
    else:
        logger.info(f"Quality acceptable, no revision needed: {reason}")
        return "end"


def should_iterate(state: ClosingBriefingState) -> Literal["critic", "end"]:
    """Determine whether to iterate (critic again) or end after revision."""
    if not state.script_revised:
        logger.warning("No revised script available, ending workflow")
        return "end"
    
    if state.iterations >= state.max_iterations:
        logger.info(f"Max iterations ({state.max_iterations}) reached, ending workflow")
        return "end"
    
    if state.critic_feedback:
        had_hallucinations = bool(state.critic_feedback.hallucinations_found)
        had_critical_failure = state.critic_feedback.hallucination_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]
        had_timeliness_issue = state.critic_feedback.timeliness_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]
        
        if had_hallucinations or had_critical_failure or had_timeliness_issue:
            logger.info(f"Re-validating after critical issue fix (iteration {state.iterations + 1}/{state.max_iterations})")
            return "critic"
    
    logger.info("Revision complete, ending workflow")
    return "end"


# ============================================================================
# Graph Construction
# ============================================================================

def build_graph() -> StateGraph:
    """
    Build and compile the LangGraph workflow for closing briefing generation.
    """
    workflow = StateGraph(ClosingBriefingState)
    
    # Add nodes - using tool-based script writer
    workflow.add_node("load_sources", load_sources_node)
    workflow.add_node("script_writer", script_writer_with_tools_node)
    workflow.add_node("critic", critic_node)
    workflow.add_node("revision_writer", revision_writer_node)
    
    # Set entry point
    workflow.set_entry_point("load_sources")
    
    # Add edges
    workflow.add_edge("load_sources", "script_writer")
    workflow.add_edge("script_writer", "critic")
    
    # Conditional edge after critic
    workflow.add_conditional_edges(
        "critic",
        should_continue_revision,
        {
            "revision_writer": "revision_writer",
            "end": END,
        }
    )
    
    # Conditional edge after revision
    workflow.add_conditional_edges(
        "revision_writer",
        should_iterate,
        {
            "critic": "critic",
            "end": END,
        }
    )
    
    app = workflow.compile()
    
    logger.info("Graph compiled successfully")
    
    return app


def save_output(state: ClosingBriefingState, output_path: str = None) -> str:
    """
    Save the final script to a file with references.
    """
    from pathlib import Path
    
    output_dir = Path(output_path) if output_path else Path(Config.OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    briefing_date = state.briefing_date or timestamp[:8]
    
    # Save final script
    final_script = state.script_revised or state.script_draft
    script_file = output_dir / f"closing_briefing_{briefing_date}_{timestamp}.md"
    
    with open(script_file, 'w', encoding='utf-8') as f:
        f.write(f"# ì¥ë§ˆê° ë¸Œë¦¬í•‘ - {briefing_date}\n\n")
        f.write(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"í‚¤ì›Œë“œ: {', '.join(state.keywords)}\n\n")
        f.write("---\n\n")
        f.write(final_script or "ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Add references section
        if state.references:
            f.write("\n\n---\n\n")
            f.write("## ğŸ“š ì°¸ê³  ìë£Œ (References)\n\n")
            
            # Group by source type
            by_type: Dict[str, List[Reference]] = {}
            for ref in state.references:
                if ref.source_type not in by_type:
                    by_type[ref.source_type] = []
                by_type[ref.source_type].append(ref)
            
            type_labels = {
                "macro_data": "ğŸ“Š ê±°ì‹œê²½ì œ ì§€í‘œ",
                "calendar_events": "ğŸ“… ê²½ì œ ì¼ì •",
                "news_data": "ğŸ“° ë‰´ìŠ¤",
                "earnings_data": "ğŸ’° ì‹¤ì  ë°œí‘œ",
                "fomc_events": "ğŸ›ï¸ FOMC",
                "market_summary": "ğŸ“ˆ ì‹œì¥ ìš”ì•½"
            }
            
            for source_type, refs in by_type.items():
                label = type_labels.get(source_type, source_type)
                f.write(f"### {label}\n\n")
                
                seen_quotes = set()
                for ref in refs:
                    quote = ref.quote
                    if quote in seen_quotes:
                        continue
                    seen_quotes.add(quote)
                    
                    line = f"- {quote}"
                    if ref.provider:
                        line += f" â€” {ref.provider}"
                    if ref.date:
                        line += f" ({ref.date})"
                    f.write(line + "\n")
                
                f.write("\n")
    
    logger.info(f"Saved script to: {script_file}")
    
    # Save metadata
    meta_file = output_dir / f"closing_briefing_{briefing_date}_{timestamp}_meta.json"
    metadata = {
        "briefing_date": state.briefing_date,
        "keywords": state.keywords,
        "iterations": state.iterations,
        "references": [ref.model_dump() for ref in state.references] if state.references else [],
        "critic_feedback": state.critic_feedback.model_dump() if state.critic_feedback else None,
        "error_message": state.error_message,
    }
    
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    return str(script_file)
