"""
LangGraph implementation for the Korean closing market briefing pipeline.

Defines nodes and graph structure for:
- load_sources: Load data from source files
- script_writer_with_tools: Generate script using tool calls for data access
- critic: Review and critique the script
- revision_writer: Revise script based on feedback
"""

import json
import logging
from typing import Literal, Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path

from openai import OpenAI
from langgraph.graph import StateGraph, END

from closing_briefing.config import Config, InvalidLLMJSONError
from closing_briefing.models import (
    ClosingBriefingState,
    ExtractedFact,
    EarningsResult,
    NewsItem,
    UpcomingEvent,
    MacroIndicator,
    CriticFeedback,
    ChecklistItem,
    HallucinationItem,
    Reference,
)
from closing_briefing.prompts import (
    SCRIPT_WRITER_WITH_TOOLS_SYSTEM_PROMPT,
    CRITIC_WITH_TOOLS_SYSTEM_PROMPT,
    REVISION_WRITER_SYSTEM_PROMPT,
)
from closing_briefing.data_loader import ClosingBriefingDataLoader
from closing_briefing.tools import BRIEFING_TOOLS, DataToolExecutor, format_tool_result_for_llm

# Import new source tools adapter (uses same tools as validation_agent)
try:
    from closing_briefing.source_tools_adapter import (
        BriefingSourceToolAdapter,
        SourceConfig,
        create_briefing_tools,
        create_tool_executor,
        format_tool_result_for_llm as format_source_tool_result,
    )
    SOURCE_TOOLS_AVAILABLE = True
except ImportError:
    SOURCE_TOOLS_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# LLM Wrapper with Tool Support
# ============================================================================

def call_llm(
    system_prompt: str,
    user_content: str,
    response_format: Literal["json", "text"] = "text",
    model: str = None,
    temperature: float = None,
) -> str:
    """
    Call the OpenAI API with the given system prompt and user content.
    """
    Config.validate()
    
    client = OpenAI(api_key=Config.OPENAI_API_KEY)
    
    model = model or Config.OPENAI_MODEL
    temperature = temperature if temperature is not None else Config.OPENAI_TEMPERATURE
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]
    
    kwargs = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": Config.MAX_TOKENS,
    }
    
    if response_format == "json":
        kwargs["response_format"] = {"type": "json_object"}
    
    try:
        logger.info(f"Calling LLM with model={model}, response_format={response_format}")
        response = client.chat.completions.create(**kwargs)
        content = response.choices[0].message.content
        
        if response_format == "json":
            try:
                json.loads(content)
            except json.JSONDecodeError as e:
                raise InvalidLLMJSONError(
                    f"LLM returned invalid JSON: {str(e)}",
                    raw_response=content
                )
        
        return content
        
    except Exception as e:
        logger.error(f"Error calling LLM: {str(e)}")
        raise


def call_llm_with_tools(
    system_prompt: str,
    user_content: str,
    tools: List[Dict],
    tool_executor: DataToolExecutor,
    model: str = None,
    temperature: float = None,
    max_tool_calls: int = 20,
) -> tuple[str, List[Reference]]:
    """
    Call the OpenAI API with tool support.
    
    The LLM can call tools to retrieve data, and we execute those tools
    and feed results back until the LLM produces a final response.
    
    Args:
        system_prompt: System prompt for the LLM
        user_content: User message content
        tools: List of tool definitions
        tool_executor: DataToolExecutor instance to execute tool calls
        model: Model to use
        temperature: Temperature setting
        max_tool_calls: Maximum number of tool call rounds
        
    Returns:
        Tuple of (final_response, list_of_references)
    """
    Config.validate()
    
    client = OpenAI(api_key=Config.OPENAI_API_KEY)
    
    model = model or Config.OPENAI_MODEL
    temperature = temperature if temperature is not None else Config.OPENAI_TEMPERATURE
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]
    
    all_references: List[Reference] = []
    tool_call_count = 0
    
    while tool_call_count < max_tool_calls:
        logger.info(f"Calling LLM with tools (round {tool_call_count + 1})")
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice="auto",
            temperature=temperature,
            max_tokens=Config.MAX_TOKENS,
        )
        
        message = response.choices[0].message
        
        # Check if the model wants to call tools
        if message.tool_calls:
            # Add assistant message with tool calls
            messages.append(message)
            
            # Process each tool call
            for tool_call in message.tool_calls:
                tool_name = tool_call.function.name
                try:
                    arguments = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError:
                    arguments = {}
                
                logger.info(f"Executing tool: {tool_name} with args: {arguments}")
                
                # Execute the tool
                result = tool_executor.execute_tool(tool_name, arguments)
                
                # Collect references
                for ref_dict in result.get('references', []):
                    all_references.append(Reference(
                        source_type=ref_dict.get('source_type', ''),
                        source_file=ref_dict.get('source_file', ''),
                        quote=ref_dict.get('quote', ''),
                        provider=ref_dict.get('provider'),
                        date=ref_dict.get('date'),
                    ))
                
                # Format result for LLM
                result_str = format_tool_result_for_llm(result)
                
                # Add tool result message
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": result_str,
                })
            
            tool_call_count += 1
        else:
            # No more tool calls, return the final response
            return message.content, all_references
    
    # If we hit max tool calls, get final response without tools
    logger.warning(f"Hit max tool calls ({max_tool_calls}), forcing final response")
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=Config.MAX_TOKENS,
    )
    
    return response.choices[0].message.content, all_references


# ============================================================================
# Node Functions
# ============================================================================

def load_sources_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Load data from source files.
    """
    logger.info("=== Node: load_sources ===")
    
    source_path = state.sources.get('_source_path', 'data/sample_sources')
    load_news_from_dynamodb = state.sources.get('_load_news_from_dynamodb', True)
    dynamodb_profile = state.sources.get('_dynamodb_profile', 'jonghyun')
    
    loader = ClosingBriefingDataLoader(
        source_path,
        load_news_from_dynamodb=load_news_from_dynamodb,
        dynamodb_profile=dynamodb_profile,
    )
    sources = loader.load_all_sources()
    
    briefing_date = state.briefing_date
    if not briefing_date:
        market_summary = sources.get('market_summary', {})
        briefing_date = market_summary.get('date', datetime.now().strftime('%Y-%m-%d'))
    
    logger.info(f"Loaded sources for briefing date: {briefing_date}")
    logger.info(f"Sources loaded: macro={len(sources.get('macro_data', []))}, "
                f"earnings={len(sources.get('earnings_data', []))}, "
                f"news={len(sources.get('news_data', []))}, "
                f"calendar={len(sources.get('calendar_events', []))}")
    
    return {
        "sources": sources,
        "briefing_date": briefing_date,
    }


def script_writer_with_tools_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Generate the closing script using tool calls to access data.
    
    The LLM will call tools to retrieve specific data and include
    exact references in the output.
    
    Uses the same source tools as validation_agent:
    - search_calendar_events: TradingEconomics calendar
    - search_macro_indicators: TradingEconomics indicators
    - search_news_articles: News from JSON/DynamoDB
    - search_fomc_events: FOMC press conferences
    - search_yahoo_finance_news: Yahoo Finance news
    - search_market_data: Live market data
    - search_sec_filings: SEC Edgar filings
    """
    logger.info("=== Node: script_writer_with_tools ===")
    
    sources = state.sources
    briefing_date = state.briefing_date
    
    # Determine which tool system to use
    use_source_tools = SOURCE_TOOLS_AVAILABLE and sources.get('_use_source_tools', False)
    
    if use_source_tools:
        # Use new source tools (same as validation_agent)
        logger.info("Using source_tools_adapter (validation_agent compatible)")
        
        config = SourceConfig(
            te_calendar_path=Path(sources.get('_te_calendar_path')) if sources.get('_te_calendar_path') else None,
            te_indicators_path=Path(sources.get('_te_indicators_path')) if sources.get('_te_indicators_path') else None,
            te_fomc_path=Path(sources.get('_te_fomc_path')) if sources.get('_te_fomc_path') else None,
            news_path=Path(sources.get('_news_path')) if sources.get('_news_path') else None,
            market_data_path=Path(sources.get('_market_data_path')) if sources.get('_market_data_path') else None,
            sec_edgar_path=Path(sources.get('_sec_edgar_path')) if sources.get('_sec_edgar_path') else None,
            yahoo_finance_profile=sources.get('_dynamodb_profile', 'jonghyun'),
            briefing_date=briefing_date,
        )
        
        tool_executor = BriefingSourceToolAdapter(config)
        tools = create_briefing_tools()
        result_formatter = format_source_tool_result
        
        user_message = f"""ì˜¤ëŠ˜ ë‚ ì§œ: {briefing_date}

ìž¥ë§ˆê° ë¸Œë¦¬í•‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ (Tools)
ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì„¸ìš”:

1. `search_calendar_events` - ê²½ì œ ì¼ì • ì¡°íšŒ (FOMC, CPI, ê³ ìš©ì§€í‘œ ë“±)
2. `search_macro_indicators` - ê±°ì‹œê²½ì œ ì§€í‘œ ì¡°íšŒ (ê¸ˆë¦¬, ë¬¼ê°€, PMI ë“±)
3. `search_news_articles` - ë‰´ìŠ¤ ê¸°ì‚¬ ì¡°íšŒ
4. `search_fomc_events` - FOMC íšŒì˜/ê¸°ìžíšŒê²¬ ì •ë³´ ì¡°íšŒ
5. `search_yahoo_finance_news` - Yahoo Finance ë‰´ìŠ¤ ì¡°íšŒ
6. `search_market_data` - ì‹œìž¥ ë°ì´í„° ì¡°íšŒ (ì§€ìˆ˜, ì£¼ê°€, VIX ë“±)
7. `search_sec_filings` - SEC ê³µì‹œ ì¡°íšŒ (10-K, 10-Q, 8-K ë“±)
8. `get_all_sources_summary` - ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ìš”ì•½ í™•ì¸

## ìž‘ì„± ê·œì¹™
1. **ë°˜ë“œì‹œ ë„êµ¬ë¥¼ ë¨¼ì € í˜¸ì¶œ**í•˜ì—¬ ë°ì´í„°ë¥¼ ì¡°íšŒí•œ í›„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìž‘ì„±í•˜ì„¸ìš”
2. ëª¨ë“  ìˆ«ìžì™€ ì‚¬ì‹¤ì€ ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ì—ì„œ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤
3. ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ [REF: source_type | "ì •í™•í•œ ì¸ìš©ë¬¸"] í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
4. ì¶”ì¸¡ì´ë‚˜ ê°€ì • ì—†ì´ ì¡°íšŒëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”

## ì¶”ì²œ ìž‘ì„± ìˆœì„œ
1. `get_all_sources_summary` - ë¨¼ì € ì–´ë–¤ ë°ì´í„°ê°€ ìžˆëŠ”ì§€ í™•ì¸
2. `search_market_data` - ì£¼ìš” ì§€ìˆ˜ ë™í–¥ í™•ì¸ (S&P 500, NASDAQ ë“±)
3. `search_news_articles` - ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ í™•ì¸
4. `search_calendar_events` - í–¥í›„ ê²½ì œ ì¼ì • í™•ì¸
5. `search_macro_indicators` - ê´€ë ¨ ê±°ì‹œì§€í‘œ í™•ì¸

ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìž‘ì„±í•˜ì„¸ìš”."""
        
    else:
        # Use legacy tool system
        logger.info("Using legacy DataToolExecutor")
        tool_executor = DataToolExecutor(sources, briefing_date)
        tools = BRIEFING_TOOLS
        result_formatter = format_tool_result_for_llm
        
        user_message = f"""ì˜¤ëŠ˜ ë‚ ì§œ: {briefing_date}

ìž¥ë§ˆê° ë¸Œë¦¬í•‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°:
- ê±°ì‹œê²½ì œ ì§€í‘œ: {len(sources.get('macro_data', []))}ê°œ
- ê²½ì œ ì¼ì •: {len(sources.get('calendar_events', []))}ê°œ
- ë‰´ìŠ¤: {len(sources.get('news_data', []))}ê°œ
- ì‹¤ì  ë°œí‘œ: {len(sources.get('earnings_data', []))}ê°œ
- FOMC ì´ë²¤íŠ¸: {len(sources.get('fomc_events', []))}ê°œ

ë„êµ¬(tools)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ , ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.
ëª¨ë“  ìˆ«ìžì™€ ì‚¬ì‹¤ì€ ë°˜ë“œì‹œ ë„êµ¬ë¥¼ í†µí•´ í™•ì¸í•œ ë°ì´í„°ì—ì„œ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.

ìŠ¤í¬ë¦½íŠ¸ ìž‘ì„± ì‹œ:
1. ë¨¼ì € get_market_summaryë¡œ ì‹œìž¥ ê°œìš”ë¥¼ í™•ì¸í•˜ì„¸ìš”
2. get_news_articlesë¡œ ì£¼ìš” ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”
3. get_earnings_resultsë¡œ ì‹¤ì  ë°œí‘œë¥¼ í™•ì¸í•˜ì„¸ìš”
4. get_calendar_eventsë¡œ í–¥í›„ ì¼ì •ì„ í™•ì¸í•˜ì„¸ìš”
5. í•„ìš”ì‹œ get_macro_indicatorsë¡œ ê±°ì‹œì§€í‘œë¥¼ í™•ì¸í•˜ì„¸ìš”

ê° ì •ë³´ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [REF: source_type | "ì •í™•í•œ ì¸ìš©ë¬¸"] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”."""

    try:
        response, references = call_llm_with_tools(
            system_prompt=SCRIPT_WRITER_WITH_TOOLS_SYSTEM_PROMPT,
            user_content=user_message,
            tools=tools,
            tool_executor=tool_executor,
        )
        
        logger.info(f"Generated script draft ({len(response)} chars) with {len(references)} references")
        
        # Extract keywords from the script (simple extraction)
        keywords = _extract_keywords_from_script(response)
        
        return {
            "script_draft": response,
            "keywords": keywords,
            "references": references,
        }
        
    except Exception as e:
        logger.error(f"Error in script_writer_with_tools: {str(e)}")
        return {
            "script_draft": None,
            "error_message": str(e),
        }


def _extract_keywords_from_script(script: str) -> List[str]:
    """Extract keywords from the script content."""
    keywords = []
    
    # Common market themes to look for
    theme_patterns = [
        ("AI", "AI"),
        ("ì¸ê³µì§€ëŠ¥", "AI"),
        ("ë°˜ë„ì²´", "ë°˜ë„ì²´"),
        ("ê¸ˆë¦¬", "ê¸ˆë¦¬"),
        ("ì—°ì¤€", "ì—°ì¤€"),
        ("Fed", "ì—°ì¤€"),
        ("ì¸í”Œë ˆì´ì…˜", "ì¸í”Œë ˆì´ì…˜"),
        ("ì‹¤ì ", "ì‹¤ì "),
        ("ê¸°ìˆ ì£¼", "ê¸°ìˆ ì£¼"),
        ("ê²½ê¸°ì¹¨ì²´", "ê²½ê¸°ì¹¨ì²´"),
        ("ê³ ìš©", "ê³ ìš©"),
    ]
    
    for pattern, keyword in theme_patterns:
        if pattern in script and keyword not in keywords:
            keywords.append(keyword)
        if len(keywords) >= 3:
            break
    
    if not keywords:
        keywords = ["ì‹œìž¥ ë™í–¥"]
    
    return keywords


def critic_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Review and critique the script draft using tool calls to verify data.
    
    Uses the same source tools as validation_agent for fact-checking.
    """
    logger.info("=== Node: critic (with tools) ===")
    
    if not state.script_draft:
        logger.error("No script draft to critique")
        return {
            "critic_feedback": None,
            "error_message": "No script draft available for critique",
        }
    
    sources = state.sources
    briefing_date = state.briefing_date
    
    # Determine which tool system to use
    use_source_tools = SOURCE_TOOLS_AVAILABLE and sources.get('_use_source_tools', False)
    
    if use_source_tools:
        # Use new source tools (same as validation_agent)
        config = SourceConfig(
            te_calendar_path=Path(sources.get('_te_calendar_path')) if sources.get('_te_calendar_path') else None,
            te_indicators_path=Path(sources.get('_te_indicators_path')) if sources.get('_te_indicators_path') else None,
            te_fomc_path=Path(sources.get('_te_fomc_path')) if sources.get('_te_fomc_path') else None,
            news_path=Path(sources.get('_news_path')) if sources.get('_news_path') else None,
            market_data_path=Path(sources.get('_market_data_path')) if sources.get('_market_data_path') else None,
            sec_edgar_path=Path(sources.get('_sec_edgar_path')) if sources.get('_sec_edgar_path') else None,
            yahoo_finance_profile=sources.get('_dynamodb_profile', 'jonghyun'),
            briefing_date=briefing_date,
        )
        tool_executor = BriefingSourceToolAdapter(config)
        tools = create_briefing_tools()
        
        user_message = f"""## ê²€ì¦í•  ëŒ€ë³¸:

{state.script_draft}

---

## ê²€ì¦ ì§€ì‹œì‚¬í•­:

ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ë³¸ì˜ ëª¨ë“  ì‚¬ì‹¤ì„ ê²€ì¦í•˜ì„¸ìš”:

1. `search_calendar_events` - ê²½ì œ ì¼ì • ê²€ì¦
2. `search_macro_indicators` - ê±°ì‹œê²½ì œ ì§€í‘œ ê²€ì¦
3. `search_news_articles` - ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ì¦
4. `search_fomc_events` - FOMC ì •ë³´ ê²€ì¦
5. `search_yahoo_finance_news` - Yahoo Finance ë‰´ìŠ¤ ê²€ì¦
6. `search_market_data` - ì‹œìž¥ ë°ì´í„° ê²€ì¦

ê²€ì¦ í”„ë¡œì„¸ìŠ¤:
1. ëŒ€ë³¸ì—ì„œ ìˆ˜ì¹˜/ë‚ ì§œ/ì´ë²¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”
2. ê° í•­ëª©ì— ëŒ€í•´ ê´€ë ¨ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”
3. ë„êµ¬ ê²°ê³¼ì™€ ëŒ€ë³¸ ë‚´ìš©ì„ ëŒ€ì¡°í•˜ì„¸ìš”
4. ë¶ˆì¼ì¹˜í•˜ëŠ” í•­ëª©ì„ "í™˜ê°"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”

í•œêµ­ì–´ë¡œ ê²€ì¦ ë³´ê³ ì„œë¥¼ ìž‘ì„±í•˜ì„¸ìš”."""
    else:
        # Use legacy tool system
        tool_executor = DataToolExecutor(sources, briefing_date)
        tools = BRIEFING_TOOLS
        
        user_message = f"""## ê²€ì¦í•  ëŒ€ë³¸:

{state.script_draft}

---

## ê²€ì¦ ì§€ì‹œì‚¬í•­:

1. ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”
2. ëŒ€ë³¸ì˜ ê° ì‚¬ì‹¤ì„ ì›ë³¸ ë°ì´í„°ì™€ ëŒ€ì¡°í•˜ì„¸ìš”
3. ë¶ˆì¼ì¹˜í•˜ê±°ë‚˜ ì¶œì²˜ê°€ ì—†ëŠ” ì •ë³´ë¥¼ í™˜ê°ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ê²€ì¦ ë³´ê³ ì„œë¥¼ ìž‘ì„±í•˜ì„¸ìš”

ê²€ì¦í•  í•­ëª©:
- ê±°ì‹œê²½ì œ ì§€í‘œ (get_macro_indicators)
- ë‰´ìŠ¤ í—¤ë“œë¼ì¸ (get_news_articles)
- ê²½ì œ ì¼ì • (get_calendar_events)
- FOMC ì •ë³´ (get_fomc_events)
- ì‹¤ì  ìˆ˜ì¹˜ (get_earnings_results)
"""

    try:
        response, _ = call_llm_with_tools(
            system_prompt=CRITIC_WITH_TOOLS_SYSTEM_PROMPT,
            user_content=user_message,
            tools=tools,
            tool_executor=tool_executor,
        )
        
        critic_feedback = _parse_critic_response(response)
        
        logger.info(f"Critic evaluation complete: {critic_feedback.overall_quality}")
        
        new_iterations = state.iterations + 1
        
        return {
            "critic_feedback": critic_feedback,
            "iterations": new_iterations,
        }
        
    except Exception as e:
        logger.error(f"Error in critic: {str(e)}")
        return {
            "critic_feedback": None,
            "error_message": str(e),
        }


def _parse_critic_response(response: str) -> CriticFeedback:
    """Parse the critic's text response into structured feedback."""
    
    # Extract summary evaluation
    summary_start = response.find("### ìš”ì•½ í‰ê°€")
    summary_end = response.find("### í•µì‹¬ ê²€ì¦ ê²°ê³¼")
    if summary_end == -1:
        summary_end = response.find("### ì²´í¬ë¦¬ìŠ¤íŠ¸")
    if summary_end == -1:
        summary_end = response.find("### ë‚´ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸")
    
    summary_evaluation = ""
    if summary_start != -1:
        if summary_end != -1:
            summary_evaluation = response[summary_start + len("### ìš”ì•½ í‰ê°€"):summary_end].strip()
        else:
            summary_evaluation = response[summary_start + len("### ìš”ì•½ í‰ê°€"):].strip()[:500]
    
    def parse_checklist_item(text: str, item_name: str) -> ChecklistItem:
        status = "ì¶©ì¡±"
        if "ì‹¬ê°í•œ ë¯¸í¡" in text:
            status = "ì‹¬ê°í•œ ë¯¸í¡"
        elif "ë¯¸í¡" in text:
            status = "ë¯¸í¡"
        
        explanation = text
        for s in ["**ì¶©ì¡±**", "**ë¯¸í¡**", "**ì‹¬ê°í•œ ë¯¸í¡**", "ì¶©ì¡±", "ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]:
            if s in explanation:
                parts = explanation.split(s, 1)
                if len(parts) > 1:
                    explanation = parts[1].strip().lstrip('-').strip()
                    break
        
        return ChecklistItem(
            item_name=item_name,
            status=status,
            explanation=explanation[:500]
        )
    
    # Extract critical validation checks
    hallucination_check = ChecklistItem(
        item_name="1. í™˜ê°(Hallucination) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    timeliness_check = ChecklistItem(
        item_name="2. ì‹œì˜ì„±(Timeliness) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    value_check = ChecklistItem(
        item_name="3. ì •ë³´ ê°€ì¹˜(Value) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    source_citation_check = ChecklistItem(
        item_name="4. ì¶œì²˜ ëª…ì‹œ(Source Citation) ê²€ì¦",
        status="ì¶©ì¡±",
        explanation="ê²€ì¦ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    
    critical_start = response.find("### í•µì‹¬ ê²€ì¦ ê²°ê³¼")
    critical_end = response.find("### ë‚´ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸")
    if critical_end == -1:
        critical_end = response.find("### ìŠ¤íƒ€ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    
    if critical_start != -1:
        if critical_end != -1:
            critical_text = response[critical_start:critical_end]
        else:
            critical_text = response[critical_start:critical_start + 1500]
        
        lines = critical_text.split('\n')
        for line in lines:
            line = line.strip()
            if 'í™˜ê°' in line or 'Hallucination' in line:
                hallucination_check = parse_checklist_item(line, "1. í™˜ê°(Hallucination) ê²€ì¦")
            elif 'ì‹œì˜ì„±' in line or 'Timeliness' in line:
                timeliness_check = parse_checklist_item(line, "2. ì‹œì˜ì„±(Timeliness) ê²€ì¦")
            elif 'ì •ë³´ ê°€ì¹˜' in line or 'Value' in line:
                value_check = parse_checklist_item(line, "3. ì •ë³´ ê°€ì¹˜(Value) ê²€ì¦")
            elif 'ì¶œì²˜' in line or 'Source Citation' in line:
                source_citation_check = parse_checklist_item(line, "4. ì¶œì²˜ ëª…ì‹œ(Source Citation) ê²€ì¦")
    
    # Extract hallucinations found
    hallucinations_found = []
    halluc_start = response.find("### í™˜ê° ë°œê²¬ ëª©ë¡")
    halluc_end = response.find("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ")
    if halluc_start != -1:
        if halluc_end != -1:
            halluc_text = response[halluc_start:halluc_end]
        else:
            halluc_text = response[halluc_start:halluc_start + 1000]
        
        if "í™˜ê° ì—†ìŒ" not in halluc_text:
            lines = halluc_text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('-') and 'â†’' in line:
                    parts = line.split('â†’')
                    if len(parts) >= 2:
                        hallucinations_found.append(HallucinationItem(
                            fabricated_content=parts[0].lstrip('-').strip(),
                            explanation=parts[1].strip()
                        ))
    
    # Extract content checklist
    checklist = []
    content_start = response.find("### ë‚´ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸")
    content_end = response.find("### ìŠ¤íƒ€ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    if content_start != -1:
        if content_end != -1:
            content_text = response[content_start:content_end]
        else:
            content_text = response[content_start:content_start + 1500]
        
        lines = content_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') and ('ì¶©ì¡±' in line or 'ë¯¸í¡' in line):
                if ':' in line:
                    parts = line.split(':', 1)
                    item_name = parts[0].replace('-', '').strip()
                    explanation = parts[1].strip() if len(parts) > 1 else ""
                    status = "ì¶©ì¡±" if "ì¶©ì¡±" in line and "ë¯¸í¡" not in line else "ë¯¸í¡"
                    checklist.append(ChecklistItem(
                        item_name=item_name,
                        status=status,
                        explanation=explanation[:500]
                    ))
    
    # Extract style checklist
    style_start = response.find("### ìŠ¤íƒ€ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸")
    style_end = response.find("### í™˜ê° ë°œê²¬ ëª©ë¡")
    if style_end == -1:
        style_end = response.find("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ")
    
    if style_start != -1:
        if style_end != -1:
            style_text = response[style_start:style_end]
        else:
            style_text = response[style_start:style_start + 1500]
        
        lines = style_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') and ('ì¶©ì¡±' in line or 'ë¯¸í¡' in line):
                if ':' in line:
                    parts = line.split(':', 1)
                    item_name = parts[0].replace('-', '').strip()
                    explanation = parts[1].strip() if len(parts) > 1 else ""
                    status = "ì¶©ì¡±" if "ì¶©ì¡±" in line and "ë¯¸í¡" not in line else "ë¯¸í¡"
                    checklist.append(ChecklistItem(
                        item_name=item_name,
                        status=status,
                        explanation=explanation[:500]
                    ))
    
    # Extract specific suggestions
    suggestions = []
    suggestions_start = response.find("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ")
    if suggestions_start != -1:
        suggestions_text = response[suggestions_start + len("### êµ¬ì²´ì ì¸ ìˆ˜ì • ì œì•ˆ"):]
        lines = suggestions_text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('-') or line.startswith('â€¢'):
                suggestion = line.lstrip('-â€¢').strip()
                if suggestion:
                    suggestions.append(suggestion)
    
    # Determine overall quality
    critical_issues = 0
    if hallucination_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        critical_issues += 2
    elif hallucination_check.status == "ë¯¸í¡":
        critical_issues += 1
    if timeliness_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        critical_issues += 2
    elif timeliness_check.status == "ë¯¸í¡":
        critical_issues += 1
    if value_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        critical_issues += 2
    elif value_check.status == "ë¯¸í¡":
        critical_issues += 1
    
    miheup_count = sum(1 for item in checklist if "ë¯¸í¡" in item.status)
    
    if critical_issues >= 2 or hallucinations_found:
        overall_quality = "ì‹¬ê°"
    elif critical_issues == 1 or miheup_count >= 4:
        overall_quality = "ë¯¸í¡"
    elif miheup_count >= 2:
        overall_quality = "ë³´í†µ"
    elif miheup_count == 1:
        overall_quality = "ì–‘í˜¸"
    else:
        overall_quality = "ìš°ìˆ˜"
    
    return CriticFeedback(
        summary_evaluation=summary_evaluation or "í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        hallucination_check=hallucination_check,
        timeliness_check=timeliness_check,
        value_check=value_check,
        source_citation_check=source_citation_check,
        hallucinations_found=hallucinations_found,
        checklist=checklist,
        specific_suggestions=suggestions,
        overall_quality=overall_quality
    )


def revision_writer_node(state: ClosingBriefingState) -> Dict[str, Any]:
    """
    Revise the script based on critic feedback using tools.
    
    Uses the same source tools as validation_agent for verification.
    """
    logger.info("=== Node: revision_writer ===")
    
    if state.critic_feedback:
        if state.critic_feedback.hallucinations_found:
            logger.warning(f"Revising to fix {len(state.critic_feedback.hallucinations_found)} hallucinations")
    
    if not state.script_draft or not state.critic_feedback:
        logger.error("Missing script draft or critic feedback for revision")
        return {
            "script_revised": state.script_draft,
            "error_message": "Missing data for revision",
        }
    
    sources = state.sources
    briefing_date = state.briefing_date
    
    # Determine which tool system to use
    use_source_tools = SOURCE_TOOLS_AVAILABLE and sources.get('_use_source_tools', False)
    
    if use_source_tools:
        # Use new source tools (same as validation_agent)
        config = SourceConfig(
            te_calendar_path=Path(sources.get('_te_calendar_path')) if sources.get('_te_calendar_path') else None,
            te_indicators_path=Path(sources.get('_te_indicators_path')) if sources.get('_te_indicators_path') else None,
            te_fomc_path=Path(sources.get('_te_fomc_path')) if sources.get('_te_fomc_path') else None,
            news_path=Path(sources.get('_news_path')) if sources.get('_news_path') else None,
            market_data_path=Path(sources.get('_market_data_path')) if sources.get('_market_data_path') else None,
            sec_edgar_path=Path(sources.get('_sec_edgar_path')) if sources.get('_sec_edgar_path') else None,
            yahoo_finance_profile=sources.get('_dynamodb_profile', 'jonghyun'),
            briefing_date=briefing_date,
        )
        tool_executor = BriefingSourceToolAdapter(config)
        tools = create_briefing_tools()
    else:
        # Use legacy tool system
        tool_executor = DataToolExecutor(sources, briefing_date)
        tools = BRIEFING_TOOLS
    
    # Build user message with feedback
    user_message = f"""## ìˆ˜ì •ì´ í•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸:

{state.script_draft}

## ë¹„í‰ í”¼ë“œë°±:

### ìš”ì•½ í‰ê°€:
{state.critic_feedback.summary_evaluation}

### ì „ì²´ í’ˆì§ˆ: {state.critic_feedback.overall_quality}

### ìˆ˜ì • ì œì•ˆ:
{chr(10).join('- ' + s for s in state.critic_feedback.specific_suggestions)}

### ë°œê²¬ëœ í™˜ê° (í—ˆìœ„ ì •ë³´):
{chr(10).join('- ' + h.fabricated_content + ' â†’ ' + h.explanation for h in state.critic_feedback.hallucinations_found) if state.critic_feedback.hallucinations_found else 'ì—†ìŒ'}

---

ìœ„ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì •ë³´ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê³ , ì •í™•í•œ ì¶œì²˜ì™€ í•¨ê»˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”."""

    try:
        response, references = call_llm_with_tools(
            system_prompt=REVISION_WRITER_SYSTEM_PROMPT,
            user_content=user_message,
            tools=tools,  # Use the appropriate tool set
            tool_executor=tool_executor,
        )
        
        logger.info(f"Generated revised script ({len(response)} chars)")
        
        # Merge new references with existing ones
        all_references = list(state.references) if state.references else []
        all_references.extend(references)
        
        return {
            "script_revised": response,
            "script_draft": response,
            "references": all_references,
        }
        
    except Exception as e:
        logger.error(f"Error in revision_writer: {str(e)}")
        return {
            "script_revised": state.script_draft,
            "error_message": str(e),
        }


# ============================================================================
# Conditional Edge Logic
# ============================================================================

def _check_needs_revision(critic_feedback: CriticFeedback) -> tuple[bool, str]:
    """Check if the script needs revision based on critic feedback."""
    if critic_feedback.hallucinations_found:
        return True, f"í™˜ê° ë°œê²¬: {len(critic_feedback.hallucinations_found)}ê°œì˜ í—ˆìœ„ ì •ë³´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    if critic_feedback.hallucination_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]:
        return True, f"í™˜ê° ê²€ì¦ ë¯¸í¡: {critic_feedback.hallucination_check.explanation}"
    
    if critic_feedback.timeliness_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]:
        return True, f"ì‹œì˜ì„± ê²€ì¦ ë¯¸í¡: {critic_feedback.timeliness_check.explanation}"
    
    if critic_feedback.value_check.status == "ì‹¬ê°í•œ ë¯¸í¡":
        return True, f"ì •ë³´ ê°€ì¹˜ ì‹¬ê°í•˜ê²Œ ë¯¸í¡: {critic_feedback.value_check.explanation}"
    
    if critic_feedback.overall_quality in ["ì‹¬ê°", "ë¯¸í¡"]:
        return True, f"ì „ì²´ í’ˆì§ˆ ë¯¸í¡: {critic_feedback.overall_quality}"
    
    critical_failures = sum(
        1 for item in critic_feedback.checklist 
        if "ë¯¸í¡" in item.status and any(
            keyword in item.item_name.lower() 
            for keyword in ["í‚¤ì›Œë“œ", "ì‹¤ì ", "ë‰´ìŠ¤", "ì´ë²¤íŠ¸"]
        )
    )
    
    if critical_failures >= 2:
        return True, f"í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© {critical_failures}ê°œ ë¯¸í¡"
    
    return False, "í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±"


def should_continue_revision(state: ClosingBriefingState) -> Literal["revision_writer", "end"]:
    """Determine whether to continue with revision or end."""
    if not state.script_draft:
        logger.warning("No script draft available, ending workflow")
        return "end"
    
    if not state.critic_feedback:
        logger.warning("No critic feedback available, ending workflow")
        return "end"
    
    if state.iterations > state.max_iterations:
        logger.info("Max iterations reached, ending workflow")
        return "end"
    
    needs_revision, reason = _check_needs_revision(state.critic_feedback)
    
    if needs_revision:
        logger.info(f"Revision needed: {reason} (iteration {state.iterations}/{state.max_iterations})")
        return "revision_writer"
    else:
        logger.info(f"Quality acceptable, no revision needed: {reason}")
        return "end"


def should_iterate(state: ClosingBriefingState) -> Literal["critic", "end"]:
    """Determine whether to iterate (critic again) or end after revision."""
    if not state.script_revised:
        logger.warning("No revised script available, ending workflow")
        return "end"
    
    if state.iterations >= state.max_iterations:
        logger.info(f"Max iterations ({state.max_iterations}) reached, ending workflow")
        return "end"
    
    if state.critic_feedback:
        had_hallucinations = bool(state.critic_feedback.hallucinations_found)
        had_critical_failure = state.critic_feedback.hallucination_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]
        had_timeliness_issue = state.critic_feedback.timeliness_check.status in ["ë¯¸í¡", "ì‹¬ê°í•œ ë¯¸í¡"]
        
        if had_hallucinations or had_critical_failure or had_timeliness_issue:
            logger.info(f"Re-validating after critical issue fix (iteration {state.iterations + 1}/{state.max_iterations})")
            return "critic"
    
    logger.info("Revision complete, ending workflow")
    return "end"


# ============================================================================
# Graph Construction
# ============================================================================

def build_graph() -> StateGraph:
    """
    Build and compile the LangGraph workflow for closing briefing generation.
    """
    workflow = StateGraph(ClosingBriefingState)
    
    # Add nodes - using tool-based script writer
    workflow.add_node("load_sources", load_sources_node)
    workflow.add_node("script_writer", script_writer_with_tools_node)
    workflow.add_node("critic", critic_node)
    workflow.add_node("revision_writer", revision_writer_node)
    
    # Set entry point
    workflow.set_entry_point("load_sources")
    
    # Add edges
    workflow.add_edge("load_sources", "script_writer")
    workflow.add_edge("script_writer", "critic")
    
    # Conditional edge after critic
    workflow.add_conditional_edges(
        "critic",
        should_continue_revision,
        {
            "revision_writer": "revision_writer",
            "end": END,
        }
    )
    
    # Conditional edge after revision
    workflow.add_conditional_edges(
        "revision_writer",
        should_iterate,
        {
            "critic": "critic",
            "end": END,
        }
    )
    
    app = workflow.compile()
    
    logger.info("Graph compiled successfully")
    
    return app


def save_output(state: ClosingBriefingState, output_path: str = None) -> str:
    """
    Save the final script to a file with references and structured sources.
    """
    from pathlib import Path
    
    output_dir = Path(output_path) if output_path else Path(Config.OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    briefing_date = state.briefing_date or timestamp[:8]
    
    # Save final script (Markdown format)
    final_script = state.script_revised or state.script_draft
    script_file = output_dir / f"closing_briefing_{briefing_date}_{timestamp}.md"
    
    with open(script_file, 'w', encoding='utf-8') as f:
        f.write(f"# ìž¥ë§ˆê° ë¸Œë¦¬í•‘ - {briefing_date}\n\n")
        f.write(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"í‚¤ì›Œë“œ: {', '.join(state.keywords)}\n\n")
        f.write("---\n\n")
        f.write(final_script or "ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Add references section
        if state.references:
            f.write("\n\n---\n\n")
            f.write("## ðŸ“š ì°¸ê³  ìžë£Œ (References)\n\n")
            
            # Group by source type
            by_type: Dict[str, List[Reference]] = {}
            for ref in state.references:
                if ref.source_type not in by_type:
                    by_type[ref.source_type] = []
                by_type[ref.source_type].append(ref)
            
            type_labels = {
                "macro_data": "ðŸ“Š ê±°ì‹œê²½ì œ ì§€í‘œ",
                "calendar_events": "ðŸ“… ê²½ì œ ì¼ì •",
                "news_data": "ðŸ“° ë‰´ìŠ¤",
                "earnings_data": "ðŸ’° ì‹¤ì  ë°œí‘œ",
                "fomc_events": "ðŸ›ï¸ FOMC",
                "market_summary": "ðŸ“ˆ ì‹œìž¥ ìš”ì•½"
            }
            
            for source_type, refs in by_type.items():
                label = type_labels.get(source_type, source_type)
                f.write(f"### {label}\n\n")
                
                seen_quotes = set()
                for ref in refs:
                    quote = ref.quote
                    if quote in seen_quotes:
                        continue
                    seen_quotes.add(quote)
                    
                    line = f"- {quote}"
                    if ref.provider:
                        line += f" â€” {ref.provider}"
                    if ref.date:
                        line += f" ({ref.date})"
                    f.write(line + "\n")
                
                f.write("\n")
    
    logger.info(f"Saved script to: {script_file}")
    
    # Convert references to structured sources format (for validation agent)
    structured_sources = _convert_references_to_sources(state.references, state.sources)
    
    # Save metadata with structured sources
    meta_file = output_dir / f"closing_briefing_{briefing_date}_{timestamp}_meta.json"
    metadata = {
        "briefing_date": state.briefing_date,
        "keywords": state.keywords,
        "iterations": state.iterations,
        "sources": structured_sources,  # Structured sources for validation
        "references": [ref.model_dump() for ref in state.references] if state.references else [],
        "critic_feedback": state.critic_feedback.model_dump() if state.critic_feedback else None,
        "error_message": state.error_message,
    }
    
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # Also save a standalone sources file for validation
    sources_file = output_dir / f"closing_briefing_{briefing_date}_{timestamp}_sources.json"
    with open(sources_file, 'w', encoding='utf-8') as f:
        json.dump({
            "date": briefing_date,
            "generated_at": datetime.now().isoformat(),
            "sources": structured_sources,
        }, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Saved sources to: {sources_file}")
    
    return str(script_file)


def _convert_references_to_sources(references: List[Reference], raw_sources: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Convert Reference objects to structured sources format for validation agent.
    
    Output format compatible with validation_agent:
    - type: "article" | "indicator" | "event" | "chart"
    - pk/id/ticker: identifier
    - title/name: human-readable name
    - date: date of the data
    - value: for indicators
    - provider: for news
    """
    structured = []
    seen_keys = set()
    
    if not references:
        return structured
    
    for ref in references:
        # Create a unique key to avoid duplicates
        key = f"{ref.source_type}:{ref.quote[:50]}"
        if key in seen_keys:
            continue
        seen_keys.add(key)
        
        source_entry = {
            "source_type": ref.source_type,
            "quote": ref.quote,
            "date": ref.date,
        }
        
        # Map to validation agent format based on source type
        if ref.source_type == "news_data":
            source_entry["type"] = "article"
            # Try to find pk from raw sources
            pk = _find_news_pk(ref.quote, raw_sources.get("news_data", []))
            if pk:
                source_entry["pk"] = pk
            source_entry["title"] = ref.quote.strip('"')
            if ref.provider:
                source_entry["provider"] = ref.provider
        
        elif ref.source_type == "calendar_events":
            source_entry["type"] = "event"
            # Extract event name and try to find id
            event_name = ref.quote.split(",")[0].strip() if ref.quote else ""
            source_entry["title"] = event_name
            event_id = _find_event_id(event_name, raw_sources.get("calendar_events", []))
            if event_id:
                source_entry["id"] = event_id
        
        elif ref.source_type == "macro_data":
            source_entry["type"] = "indicator"
            # Parse name and value from quote
            parts = ref.quote.split(":")
            if len(parts) >= 2:
                source_entry["name"] = parts[0].strip()
                source_entry["value"] = parts[1].strip().split(",")[0]
        
        elif ref.source_type == "market_summary":
            source_entry["type"] = "chart"
            # Try to extract ticker from quote
            ticker = _extract_ticker_from_quote(ref.quote)
            if ticker:
                source_entry["ticker"] = ticker
        
        elif ref.source_type == "earnings_data":
            source_entry["type"] = "earnings"
            # Parse company info
            parts = ref.quote.split(",")
            if parts:
                company_part = parts[0].strip()
                source_entry["company"] = company_part
                # Extract ticker if present
                if "(" in company_part and ")" in company_part:
                    ticker = company_part.split("(")[1].split(")")[0]
                    source_entry["ticker"] = ticker
        
        elif ref.source_type == "fomc_events":
            source_entry["type"] = "fomc"
            source_entry["title"] = ref.quote.split(",")[0].strip() if ref.quote else ""
        
        structured.append(source_entry)
    
    return structured


def _find_news_pk(quote: str, news_data: List[Dict]) -> Optional[str]:
    """Find the pk for a news article by matching headline."""
    headline = quote.strip('"')
    for article in news_data:
        article_headline = article.get("headline", "") or article.get("title", "")
        if headline.lower() in article_headline.lower() or article_headline.lower() in headline.lower():
            return article.get("pk") or article.get("id")
    return None


def _find_event_id(event_name: str, calendar_events: List[Dict]) -> Optional[str]:
    """Find the id for a calendar event by matching name."""
    for event in calendar_events:
        name = event.get("name", "") or event.get("title", "")
        if event_name.lower() in name.lower() or name.lower() in event_name.lower():
            return event.get("id")
    return None


def _extract_ticker_from_quote(quote: str) -> Optional[str]:
    """Extract ticker symbol from market summary quote."""
    # Common index mappings
    index_map = {
        "s&p 500": "^GSPC",
        "nasdaq": "^IXIC",
        "dow": "^DJI",
        "russell 2000": "^RUT",
        "vix": "^VIX",
    }
    
    quote_lower = quote.lower()
    for name, ticker in index_map.items():
        if name in quote_lower:
            return ticker
    
    return None
