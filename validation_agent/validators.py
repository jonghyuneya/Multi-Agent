"""
Validators for the Validation Agent.

This module provides concrete implementations of Validator for different
validation types:
1. FactValidator - Validates factual claims against source data
2. AudienceValidator - Checks content fitness for target audience
3. CitationValidator - Ensures all claims have proper citations

Uses LLM with tool calling for intelligent validation.
"""

from __future__ import annotations

import json
import logging
import os
import re
from typing import Any, Dict, List, Optional

from openai import OpenAI

from validation_agent.base import (
    Validator,
    ValidationResult,
    SourceMatch,
    ValidationStatus,
    AudienceFitness,
    SourceTool,
)

logger = logging.getLogger(__name__)


# =============================================================================
# LLM Helper
# =============================================================================

def call_llm_with_tools(
    messages: List[Dict[str, Any]],
    tools: List[Dict[str, Any]],
    source_tools: Dict[str, SourceTool],
    max_tool_iterations: int = 10,
    model: str = None,
) -> str:
    """
    Call LLM with tool support, handling tool calls iteratively.
    
    Args:
        messages: Conversation messages
        tools: Tool definitions for function calling
        source_tools: Registered source tools for execution
        max_tool_iterations: Maximum number of tool call iterations
        model: LLM model to use
        
    Returns:
        Final LLM response text
    """
    client = OpenAI()
    model = model or os.getenv("OPENAI_MODEL", "gpt-4o")
    
    for iteration in range(max_tool_iterations):
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools if tools else None,
            tool_choice="auto" if tools else None,
            temperature=0.1,  # Low temperature for factual validation
        )
        
        assistant_message = response.choices[0].message
        messages.append(assistant_message.model_dump())
        
        # Check for tool calls
        if assistant_message.tool_calls:
            for tool_call in assistant_message.tool_calls:
                tool_name = tool_call.function.name
                tool_args = json.loads(tool_call.function.arguments)
                
                # Execute tool
                result = execute_source_tool(tool_name, tool_args, source_tools)
                
                # Add tool result to messages
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result, ensure_ascii=False),
                })
        else:
            # No more tool calls, return response
            return assistant_message.content or ""
    
    logger.warning(f"Max tool iterations ({max_tool_iterations}) reached")
    return messages[-1].get("content", "") if messages else ""


def execute_source_tool(
    tool_name: str,
    arguments: Dict[str, Any],
    source_tools: Dict[str, SourceTool],
) -> Dict[str, Any]:
    """
    Execute a source tool call.
    
    Maps tool names to source types:
    - search_calendar_events -> calendar_events
    - search_macro_data -> macro_data
    - search_fomc_events -> fomc_events
    - search_news_data -> news_data
    """
    # Map tool names to source types
    tool_to_source = {
        "search_calendar_events": "calendar_events",
        "search_macro_data": "macro_data",
        "search_fomc_events": "fomc_events",
        "search_news_data": "news_data",
    }
    
    source_type = tool_to_source.get(tool_name)
    
    if not source_type:
        # Try extracting from tool name pattern "search_{source_type}"
        if tool_name.startswith("search_"):
            source_type = tool_name[7:]
    
    if not source_type or source_type not in source_tools:
        return {"error": f"Unknown tool or source: {tool_name}"}
    
    tool = source_tools[source_type]
    query = arguments.get("query", "")
    
    try:
        results = tool.search(query)
        return {
            "source_type": source_type,
            "query": query,
            "results": results[:10],  # Limit results
            "count": len(results),
        }
    except Exception as e:
        logger.error(f"Error executing {tool_name}: {e}")
        return {"error": str(e)}


# =============================================================================
# Fact Validator
# =============================================================================

FACT_VALIDATOR_PROMPT = """ë‹¹ì‹ ì€ AI ìƒì„± ìŠ¤í¬ë¦½íŠ¸ì˜ ì‚¬ì‹¤ ì •í™•ì„±ì„ ê²€ì¦í•˜ëŠ” ê²€ì¦ ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

## ì—­í• 
ìŠ¤í¬ë¦½íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ì‚¬ì‹¤ì  ì£¼ìž¥(claim)ì´ ì›ë³¸ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

## ê²€ì¦ ê¸°ì¤€
1. **ìˆ˜ì¹˜ ì •í™•ì„±**: ìˆ«ìž, ë°±ë¶„ìœ¨, ë‚ ì§œê°€ ì¶œì²˜ ë°ì´í„°ì™€ ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ”ê°€?
2. **ë§¥ë½ ì •í™•ì„±**: ì •ë³´ê°€ ì˜¬ë°”ë¥¸ ë§¥ë½ì—ì„œ ì‚¬ìš©ë˜ì—ˆëŠ”ê°€?
3. **í•´ì„ ì •í™•ì„±**: ë°ì´í„° í•´ì„ì´ í•©ë¦¬ì ì¸ê°€?
4. **ì¶œì²˜ ì¡´ìž¬**: ì¸ìš©ëœ ì¶œì²˜ê°€ ì‹¤ì œë¡œ ì¡´ìž¬í•˜ëŠ”ê°€?

## ë„êµ¬ ì‚¬ìš©
ë‹¤ìŒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì‹¤ì„ ê²€ì¦í•˜ì„¸ìš”:
- search_calendar_events: ê²½ì œ ìº˜ë¦°ë” ì´ë²¤íŠ¸ ê²€ìƒ‰
- search_macro_data: ê±°ì‹œê²½ì œ ì§€í‘œ ê²€ìƒ‰
- search_fomc_events: FOMC ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
- search_news_data: ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰

## ì¶œë ¥ í˜•ì‹
ê²€ì¦ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

```json
{
    "claims": [
        {
            "claim_text": "ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì¶”ì¶œí•œ ì£¼ìž¥",
            "source_type": "calendar_events|macro_data|fomc_events|news_data",
            "reference": "ì¸ìš©ëœ ì¶œì²˜ ì •ë³´",
            "status": "valid|partial|invalid|not_found",
            "confidence": 0.0-1.0,
            "explanation": "ê²€ì¦ ê²°ê³¼ ì„¤ëª…",
            "suggested_correction": "ìˆ˜ì • ì œì•ˆ (invalidì¸ ê²½ìš°)"
        }
    ],
    "summary": "ì „ì²´ ê²€ì¦ ìš”ì•½"
}
```

ê²€ì¦í•  ìŠ¤í¬ë¦½íŠ¸:
"""

class FactValidator(Validator):
    """
    Validates factual claims in scripts against source data.
    
    Uses LLM with tool calling to:
    1. Extract claims from the script
    2. Identify the source type for each claim
    3. Search source data to verify claims
    4. Report validation results
    """
    
    @property
    def validator_type(self) -> str:
        return "fact"
    
    def validate(
        self,
        script: str,
        source_tools: Dict[str, SourceTool],
        **kwargs
    ) -> ValidationResult:
        """Validate factual claims in the script."""
        result = ValidationResult(script_id=kwargs.get("script_id", "unknown"))
        
        # Get tool definitions
        tools = [tool.get_tool_definition() for tool in source_tools.values()]
        
        # Build messages
        messages = [
            {"role": "system", "content": FACT_VALIDATOR_PROMPT},
            {"role": "user", "content": script},
        ]
        
        try:
            # Call LLM with tools
            response = call_llm_with_tools(messages, tools, source_tools)
            
            # Parse response
            validation_data = self._parse_validation_response(response)
            
            # Convert to ValidationResult
            for claim_data in validation_data.get("claims", []):
                status = self._map_status(claim_data.get("status", "not_found"))
                
                source_match = SourceMatch(
                    claim=claim_data.get("claim_text", ""),
                    source_type=claim_data.get("source_type", "unknown"),
                    source_reference=claim_data.get("reference", ""),
                    status=status,
                    confidence=claim_data.get("confidence", 0.0),
                    explanation=claim_data.get("explanation", ""),
                    suggested_correction=claim_data.get("suggested_correction"),
                )
                
                result.source_matches.append(source_match)
                result.total_claims += 1
                
                if status == ValidationStatus.VALID:
                    result.valid_claims += 1
                elif status == ValidationStatus.INVALID:
                    result.invalid_claims += 1
                elif status == ValidationStatus.NOT_FOUND:
                    result.not_found_claims += 1
            
            result.summary = validation_data.get("summary", "")
            
        except Exception as e:
            logger.error(f"FactValidator error: {e}")
            result.errors.append(str(e))
        
        return result
    
    def _parse_validation_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response to extract validation data."""
        # Try to extract JSON from response
        json_match = re.search(r"```json\s*([\s\S]*?)\s*```", response)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Try parsing entire response as JSON
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass
        
        # Return empty if parsing fails
        logger.warning("Could not parse validation response as JSON")
        return {"claims": [], "summary": response}
    
    def _map_status(self, status_str: str) -> ValidationStatus:
        """Map status string to ValidationStatus enum."""
        mapping = {
            "valid": ValidationStatus.VALID,
            "partial": ValidationStatus.PARTIAL,
            "invalid": ValidationStatus.INVALID,
            "not_found": ValidationStatus.NOT_FOUND,
            "error": ValidationStatus.ERROR,
        }
        return mapping.get(status_str.lower(), ValidationStatus.NOT_FOUND)


# =============================================================================
# Audience Validator
# =============================================================================

AUDIENCE_VALIDATOR_PROMPT = """ë‹¹ì‹ ì€ AI ìƒì„± ìŠ¤í¬ë¦½íŠ¸ì˜ ëŒ€ìƒ ì í•©ì„±ì„ í‰ê°€í•˜ëŠ” ê²€ì¦ ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

## ëŒ€ìƒ ë…ìž
ê²½ì œ ë‰´ìŠ¤ì™€ ì£¼ì‹ ì‹œìž¥ì— ê´€ì‹¬ì´ ìžˆëŠ” íˆ¬ìžìžë“¤

## í‰ê°€ ê¸°ì¤€

### 1. ê´€ë ¨ì„± (Relevance)
- ë‚´ìš©ì´ ê²½ì œ/ê¸ˆìœµ ë‰´ìŠ¤ì™€ ê´€ë ¨ì´ ìžˆëŠ”ê°€?
- íˆ¬ìž ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ì¸ê°€?
- ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìžˆëŠ”ê°€?

### 2. ì ‘ê·¼ì„± (Accessibility)
- ì „ë¬¸ ìš©ì–´ê°€ ì ì ˆížˆ ì„¤ëª…ë˜ì–´ ìžˆëŠ”ê°€?
- ì¼ë°˜ íˆ¬ìžìžë„ ì´í•´í•  ìˆ˜ ìžˆëŠ” ìˆ˜ì¤€ì¸ê°€?
- ë„ˆë¬´ ê¸°ì´ˆì ì´ê±°ë‚˜ ë„ˆë¬´ ì „ë¬¸ì ì´ì§€ ì•Šì€ê°€?

### 3. ì‹¤ìš©ì„± (Actionability)
- ì •ë³´ê°€ íˆ¬ìž íŒë‹¨ì— ë„ì›€ì´ ë˜ëŠ”ê°€?
- ì‹œìž¥ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•˜ëŠ”ê°€?
- ë‹¨ìˆœ ë°ì´í„° ë‚˜ì—´ì´ ì•„ë‹Œ ë¶„ì„ì´ ìžˆëŠ”ê°€?

### 4. ì–´ì¡° (Tone)
- ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ ì–´ì¡°ì¸ê°€?
- ê³¼ìž¥ë˜ê±°ë‚˜ ì„ ì •ì ì´ì§€ ì•Šì€ê°€?
- ê· í˜• ìž¡ížŒ ì‹œê°ì„ ì œê³µí•˜ëŠ”ê°€?

### 5. êµ¬ì¡° (Structure)
- ë…¼ë¦¬ì ì¸ íë¦„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆëŠ”ê°€?
- ì¤‘ìš”í•œ ì •ë³´ê°€ ëª…í™•í•˜ê²Œ ì „ë‹¬ë˜ëŠ”ê°€?
- ì²­ì·¨ìžê°€ ë”°ë¼ê°€ê¸° ì‰¬ìš´ê°€?

## ì¶œë ¥ í˜•ì‹
í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

```json
{
    "fitness": "excellent|good|fair|poor",
    "scores": {
        "relevance": 1-5,
        "accessibility": 1-5,
        "actionability": 1-5,
        "tone": 1-5,
        "structure": 1-5
    },
    "strengths": ["ìž¥ì  1", "ìž¥ì  2"],
    "improvements": ["ê°œì„ ì  1", "ê°œì„ ì  2"],
    "specific_issues": [
        {
            "location": "ë¬¸ì œê°€ ìžˆëŠ” ë¶€ë¶„ ì¸ìš©",
            "issue": "ë¬¸ì œ ì„¤ëª…",
            "suggestion": "ê°œì„  ì œì•ˆ"
        }
    ],
    "summary": "ì „ì²´ í‰ê°€ ìš”ì•½"
}
```

í‰ê°€í•  ìŠ¤í¬ë¦½íŠ¸:
"""

class AudienceValidator(Validator):
    """
    Validates content fitness for target audience.
    
    Evaluates:
    - Relevance to economic/financial news
    - Accessibility for general investors
    - Actionability of information
    - Professional but approachable tone
    - Logical structure and flow
    """
    
    @property
    def validator_type(self) -> str:
        return "audience"
    
    def validate(
        self,
        script: str,
        source_tools: Dict[str, SourceTool],
        **kwargs
    ) -> ValidationResult:
        """Validate audience fitness of the script."""
        result = ValidationResult(script_id=kwargs.get("script_id", "unknown"))
        
        # Build messages (no tools needed for audience validation)
        messages = [
            {"role": "system", "content": AUDIENCE_VALIDATOR_PROMPT},
            {"role": "user", "content": script},
        ]
        
        try:
            client = OpenAI()
            model = os.getenv("OPENAI_MODEL", "gpt-4o")
            
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,
            )
            
            response_text = response.choices[0].message.content or ""
            
            # Parse response
            evaluation = self._parse_evaluation_response(response_text)
            
            # Map fitness
            fitness_str = evaluation.get("fitness", "good")
            result.audience_fitness = self._map_fitness(fitness_str)
            
            # Build feedback
            feedback_parts = []
            
            if evaluation.get("strengths"):
                feedback_parts.append("âœ… ìž¥ì :\n" + "\n".join(f"  - {s}" for s in evaluation["strengths"]))
            
            if evaluation.get("improvements"):
                feedback_parts.append("âš ï¸ ê°œì„ ì :\n" + "\n".join(f"  - {i}" for i in evaluation["improvements"]))
            
            if evaluation.get("specific_issues"):
                feedback_parts.append("ðŸ“ êµ¬ì²´ì  ì´ìŠˆ:")
                for issue in evaluation["specific_issues"]:
                    feedback_parts.append(f"  ìœ„ì¹˜: {issue.get('location', '?')}")
                    feedback_parts.append(f"  ë¬¸ì œ: {issue.get('issue', '?')}")
                    feedback_parts.append(f"  ì œì•ˆ: {issue.get('suggestion', '?')}")
            
            result.audience_feedback = "\n\n".join(feedback_parts)
            result.summary = evaluation.get("summary", "")
            
        except Exception as e:
            logger.error(f"AudienceValidator error: {e}")
            result.errors.append(str(e))
        
        return result
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response to extract evaluation data."""
        json_match = re.search(r"```json\s*([\s\S]*?)\s*```", response)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass
        
        return {"fitness": "good", "summary": response}
    
    def _map_fitness(self, fitness_str: str) -> AudienceFitness:
        """Map fitness string to AudienceFitness enum."""
        mapping = {
            "excellent": AudienceFitness.EXCELLENT,
            "good": AudienceFitness.GOOD,
            "fair": AudienceFitness.FAIR,
            "poor": AudienceFitness.POOR,
        }
        return mapping.get(fitness_str.lower(), AudienceFitness.GOOD)


# =============================================================================
# Citation Validator
# =============================================================================

CITATION_VALIDATOR_PROMPT = """ë‹¹ì‹ ì€ AI ìƒì„± ìŠ¤í¬ë¦½íŠ¸ì˜ ì¸ìš© ì™„ì „ì„±ì„ ê²€ì¦í•˜ëŠ” ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

## ì—­í• 
ìŠ¤í¬ë¦½íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ì‚¬ì‹¤ì  ì£¼ìž¥ì— ì ì ˆí•œ ì¶œì²˜ í‘œê¸°ê°€ ìžˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

## ì¸ìš©ì´ í•„ìš”í•œ ì •ë³´ ìœ í˜•
1. **ìˆ˜ì¹˜ ë°ì´í„°**: ê²½ì œ ì§€í‘œ, ê°€ê²©, ë³€ë™ë¥  ë“±
2. **ì´ë²¤íŠ¸ ì •ë³´**: FOMC ê²°ì •, ê²½ì œ ì§€í‘œ ë°œí‘œ ë“±
3. **ì¸ìš©ë¬¸**: ì—°ì¤€ ì˜ìž¥ ë°œì–¸, ë¶„ì„ê°€ ì½”ë©˜íŠ¸ ë“±
4. **ë‰´ìŠ¤ ë‚´ìš©**: íŠ¹ì • ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©
5. **ì˜ˆì¸¡/ì „ë§**: ê³µì‹ì ì¸ ì˜ˆì¸¡ì´ë‚˜ ì „ë§ì¹˜

## ì¸ìš© í˜•ì‹
ì˜¬ë°”ë¥¸ ì¸ìš© í˜•ì‹: [REF: source_type | "ì¸ìš©ë¬¸"]

ì˜ˆì‹œ:
- [REF: macro_data | "ë¯¸êµ­ CPI 3.0%"]
- [REF: fomc_events | "ì—°ì¤€, ê¸ˆë¦¬ ë™ê²° ê²°ì •"]
- [REF: news_data | "ì• í”Œ, ì‹ ì œí’ˆ ë°œí‘œ"]

## ì¶œë ¥ í˜•ì‹
ê²€ì¦ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

```json
{
    "citations_complete": true|false,
    "citation_count": 0,
    "claims_without_citation": [
        {
            "claim": "ì¸ìš© ì—†ëŠ” ì£¼ìž¥",
            "suggested_source_type": "calendar_events|macro_data|fomc_events|news_data",
            "reason": "ì¸ìš©ì´ í•„ìš”í•œ ì´ìœ "
        }
    ],
    "citation_issues": [
        {
            "citation": "ë¬¸ì œê°€ ìžˆëŠ” ì¸ìš©",
            "issue": "ë¬¸ì œ ì„¤ëª…",
            "suggestion": "ê°œì„  ì œì•ˆ"
        }
    ],
    "summary": "ì „ì²´ ê²€ì¦ ìš”ì•½"
}
```

ê²€ì¦í•  ìŠ¤í¬ë¦½íŠ¸:
"""

class CitationValidator(Validator):
    """
    Validates that all factual claims have proper citations.
    
    Checks:
    - All numerical data has citations
    - Event information has citations
    - Quotes are properly attributed
    - Citation format is correct
    """
    
    @property
    def validator_type(self) -> str:
        return "citation"
    
    def validate(
        self,
        script: str,
        source_tools: Dict[str, SourceTool],
        **kwargs
    ) -> ValidationResult:
        """Validate citation completeness in the script."""
        result = ValidationResult(script_id=kwargs.get("script_id", "unknown"))
        
        # Build messages
        messages = [
            {"role": "system", "content": CITATION_VALIDATOR_PROMPT},
            {"role": "user", "content": script},
        ]
        
        try:
            client = OpenAI()
            model = os.getenv("OPENAI_MODEL", "gpt-4o")
            
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
            )
            
            response_text = response.choices[0].message.content or ""
            
            # Parse response
            validation_data = self._parse_citation_response(response_text)
            
            # Update result
            result.citations_complete = validation_data.get("citations_complete", True)
            
            # Extract missing citations
            for claim in validation_data.get("claims_without_citation", []):
                result.missing_citations.append(claim.get("claim", ""))
            
            result.summary = validation_data.get("summary", "")
            
        except Exception as e:
            logger.error(f"CitationValidator error: {e}")
            result.errors.append(str(e))
        
        return result
    
    def _parse_citation_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response to extract citation validation data."""
        json_match = re.search(r"```json\s*([\s\S]*?)\s*```", response)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass
        
        return {"citations_complete": True, "summary": response}

